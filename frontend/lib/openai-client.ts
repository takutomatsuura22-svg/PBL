/**
 * OpenAI API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
 * ChatGPT APIã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªAIæ©Ÿèƒ½
 */

import OpenAI from 'openai'

// OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
let openaiClient: OpenAI | null = null

/**
 * OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
 */
export function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    const apiKey = process.env.OPENAI_API_KEY
    
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY is not configured in environment variables')
    }
    
    openaiClient = new OpenAI({
      apiKey: apiKey,
    })
  }
  
  return openaiClient
}

/**
 * OpenAI APIãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
 */
export function isOpenAIEnabled(): boolean {
  return !!process.env.OPENAI_API_KEY
}

/**
 * ChatGPTã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¦å¿œç­”ã‚’å–å¾—
 */
export async function chatCompletion(
  messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>,
  options?: {
    model?: string
    temperature?: number
    max_tokens?: number
  }
): Promise<string> {
  const client = getOpenAIClient()
  
  // ã‚³ã‚¹ãƒˆå‰Šæ¸›: max_tokensã‚’é©åˆ‡ã«åˆ¶é™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ800ï¼‰
  const maxTokens = options?.max_tokens ?? 800
  
  const response = await client.chat.completions.create({
    model: options?.model || process.env.OPENAI_MODEL || 'gpt-4o', // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯gpt-4oï¼ˆé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼‰
    messages: messages,
    temperature: options?.temperature ?? 0.7,
    max_tokens: maxTokens,
  })
  
  // ä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰
  if (process.env.NODE_ENV === 'development') {
    const usage = response.usage
    if (usage) {
      console.log(`ğŸ“Š OpenAI APIä½¿ç”¨é‡: ${usage.total_tokens}ãƒˆãƒ¼ã‚¯ãƒ³ (å…¥åŠ›: ${usage.prompt_tokens}, å‡ºåŠ›: ${usage.completion_tokens})`)
    }
  }
  
  return response.choices[0]?.message?.content || ''
}

/**
 * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ChatGPTã®å¿œç­”ã‚’å–å¾—
 */
export async function* chatCompletionStream(
  messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>,
  options?: {
    model?: string
    temperature?: number
    max_tokens?: number
  }
): AsyncGenerator<string> {
  const client = getOpenAIClient()
  
  const stream = await client.chat.completions.create({
    model: options?.model || process.env.OPENAI_MODEL || 'gpt-4o', // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯gpt-4oï¼ˆé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼‰
    messages: messages,
    temperature: options?.temperature ?? 0.7,
    max_tokens: options?.max_tokens ?? 1000,
    stream: true,
  })
  
  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content || ''
    if (content) {
      yield content
    }
  }
}

